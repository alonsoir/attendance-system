services:
  consul:
    image: hashicorp/consul:1.20
    env_file:
      - .env
    environment:
      - CONSUL_LOCAL_CONFIG={"verify_incoming":false}
    ports:
      - "8500:8500"
    networks:
      - default
    volumes:
      - consul-data:/consul/data
    command: agent -server -bootstrap-expect=1 -ui -bind=0.0.0.0 -client=0.0.0.0 -advertise=10.0.0.2
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8500/v1/status/leader || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5

  vault:
      image: hashicorp/vault:1.18
      container_name: vault
      restart: always
      env_file:
        - .env
      ports:
        - "8200:8200"
      environment:
        VAULT_API_ADDR: http://vault:8200
        VAULT_CLUSTER_ADDR: https://vault:8201
        VAULT_STORAGE: consul
        VAULT_BACKEND: consul
        VAULT_STORAGE_CONSUL_ADDRESS: http://consul:8500
      volumes:
        - ${PROJECT_ROOT}/backend/scripts:/scripts:ro
        - vault-data:/vault/data
      command: sh -c "sleep 10 && vault server -config=/scripts/vault.hcl"
      depends_on:
        - consul
      healthcheck:
        test: [ "CMD", "vault", "status" ]
        interval: 10s
        timeout: 5s
        retries: 3
      cap_add:
        - IPC_LOCK
      networks:
        - default

  vault-init:
      image: alpine:latest
      env_file:
        - .env
      depends_on:
        - vault
      environment:
        - VAULT_ADDR=http://vault:8200
        - CONSUL_ADDR=http://consul:8500
      networks:
        - default
      volumes:
        - .env:/app/.env:ro
        - /Users/aironman/git/attendance_system/backend/scripts:/scripts
        - vault-data:/vault/file
        - /Users/aironman/git/attendance_system/vault-init:/vault-init
      command: sh -c "chmod +x /scripts/simple-init-vault.sh && /scripts/simple-init-vault.sh"
  front:
    build:
      context: .
      dockerfile: attendance_system/frontend/Dockerfile
    image: frontend-attendance
    env_file:
      - .env
    environment:
      - VAULT_ADDR=http://vault:8200
      - VAULT_TOKEN=attendance_root_token
      - VITE_API_URL=http://back:8000
    networks:
      - default
    depends_on:
      - back
      - consul
      - vault
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health", "||", "exit", "1"]
      interval: 10s
      timeout: 5s
      retries: 3
    deploy:
      replicas: 1
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      - ./logs:/app/logs:rw

  back:
    build:
      context: .
      dockerfile: attendance_system/backend/Dockerfile
    image: backend-attendance
    env_file:
      - .env
    environment:
      - POSTGRES_SERVER=db
      - POSTGRES_PORT=5432
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - VAULT_ADDR=http://vault:8200
      - VAULT_TOKEN=attendance_root_token
      - PROJECT_NAME=${PROJECT_NAME}
      - PROJECT_DESCRIPTION=${PROJECT_DESCRIPTION}
      - VERSION=${VERSION}
      - API_V1_STR=${API_V1_STR}
      - BACKEND_CORS_ORIGINS=${BACKEND_CORS_ORIGINS}
      - ENABLE_METRICS=${ENABLE_METRICS}
      - SECRET_KEY=${SECRET_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    networks:
      - default
    depends_on:
      - vault
      - consul
      - db
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health", "||", "exit", "1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      replicas: 1
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      - ./logs:/app/logs:rw

  db:
    image: test-postgres-encrypted:latest
    env_file:
      - .env
    environment:
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_password
      POSTGRES_DB: test_db
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 768MB
      POSTGRES_WORK_MEM: 16MB
    command: postgres -c "huge_pages=off"
    networks:
      - default
    ports:
      - "5432:5432"
    volumes:
      - db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U test_user -d test_db"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      replicas: 1
      resources:
        limits:
          memory: '1G'
        reservations:
          memory: '512M'
    depends_on:
      - consul
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Coordinador
  coordinator:
    image: test-postgres-full-citus:latest
    environment:
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_password
      POSTGRES_DB: test_db
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 768MB
      POSTGRES_WORK_MEM: 16MB
      POSTGRES_MAINTENANCE_WORK_MEM: 64MB   # Para operaciones de mantenimiento
      # Añadir variables para rendimiento
      POSTGRES_MAX_CONNECTIONS: 100
      POSTGRES_MAX_WORKER_PROCESSES: 8
      POSTGRES_MAX_PARALLEL_WORKERS: 8
      POSTGRES_MAX_PARALLEL_WORKERS_PER_GATHER: 4
    ports:
      - "5432:5432"
    volumes:
      - coordinator_data:/var/lib/postgresql/data
    depends_on:
      - consul
    deploy:
      replicas: 1
      resources:
        limits:
          # BALANCEADA Puede usar hasta 1.5 núcleos. Equivalente a cpu_quota: 150000, cpu_period: 100000
          # cpus: '1.5'
          # memory: '1G'
          cpus: '1.5'
          # AGRESIVA Puede usar hasta 2 núcleos. eEquivalente a cpu_quota: 200000, cpu_period: 100000
          # cpus: '2.0'
          # memory: '4G'
          # Consecuencias:
          # Ideal para operaciones intensivas de cifrado
          # Óptimo para muchas operaciones concurrentes
          # Puede impactar negativamente otros servicios
          # Mayor consumo de energía

          # CONSERVADORA.
          # cpus: '0.5'      # Equivalente a cpu_quota: 50000, cpu_period: 100000
          # memory: '512M'
          # Menor rendimiento en operaciones pesadas
          # Mejor para entornos compartidos
          # Menor impacto en la factura del cloud
          # Puede causar cuellos de botella en picos de carga

          # BALANCEADA.
          # Buen equilibrio rendimiento/recursos
          # Maneja bien picos ocasionales
          # Permite otras cargas en el sistema
          # Costo moderado en cloud

          # DESARROLLO.
          # cpus: '0.8'     # Garantiza el 80% de un núcleo
          # memory: '768M'
          # ALTA DISPONIBILIDAD.
          # cpus: '3.0'     # Equivalente a cpu_quota: 300000, cpu_period: 100000
          # memory: '8G'

          # microservicios
          # cpus: '0.3'      # Equivalente a cpu_quota: 30000, cpu_period: 100000
          # memory: '256M'
          # Ideal para arquitecturas distribuidas
          # Permite más instancias por máquina
          # Menor rendimiento individual
          # Mejor para escalado horizontal

          # análisis de datos
          # cpus: '4.0'      # Equivalente a cpu_quota: 400000, cpu_period: 100000
          # memory: '16G'
          # Óptimo para queries analíticas
          # Excelente para agregaciones
          # Muy costoso en recursos
          # Requiere hardware potente

          # entornos compartidos
          # cpus: '1.0'      # Equivalente a cpu_quota: 100000, cpu_period: 100000
          # memory: '1.5G'
          # Buen vecino en sistemas compartidos
          # Rendimiento predecible
          # Balance costo/rendimiento
          # Adecuado para la mayoría de casos
          memory: '1G'
        reservations:
          # AGRESIVA Puede usar hasta 1 núcleos. eEquivalente a cpu_quota: 100000, cpu_period: 100000
          # cpus: '1.0'
          # memory: '2G

          # CONSERVADORA.
          # cpus: '0.25'
          # memory: '256M'
          # Garantiza 1/4 de núcleo

          # BALANCEADA Garantiza mínimo medio núcleo
          # cpus: '0.5'
          # memory: '512M'

          # DESARROLLO.
          # cpus: '0.2'     # Garantiza el 80% de un núcleo
          # memory: '384M'

          # ALTA DISPONIBILIDAD.
          # cpus: '1.5'     # # Garantiza núcleo y medio
          # memory: '4G'
          # Excelente para cargas críticas
          # Ideal para bases de datos grandes
          # Costoso en entornos cloud
          # Puede requerir hardware específico

          # MICROSERVICIOS.
          # cpus: '0.1'      # Garantiza 1/10 de núcleo
          # memory: '128M'

          # análisis de datos
          # cpus: '2.0'      # Garantiza 2 núcleos
          # memory: '8G'

          # entornos compartidos
          # cpus: '0.3'      # Equivalente a cpu_quota: 100000, cpu_period: 100000
          # memory: '768M'
          cpus: '0.5'
          memory: '512M'
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        delay: 120s        # Esperar 2 minutos entre actualizaciones
        order: start-first # Iniciar nuevo contenedor antes de parar el antiguo
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: >
      postgres 
      -c config_file=/etc/postgresql/postgresql.conf
      -c maintenance_work_mem=128MB
      -c synchronous_commit=off
      -c checkpoint_timeout=30min
      -c max_wal_size=2GB
    healthcheck:
      test: >
        CMD-SHELL
        pg_isready -U test_user -d test_db &&
        psql -U test_user -d test_db -c "SELECT 1 FROM citus_dist_local_group" &&
        psql -U test_user -d test_db -c "SELECT * FROM citus_health_check()"
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s  # Dar tiempo suficiente para el inicio inicial

  # Workers
  worker:
    image: test-postgres-full-citus:latest
    environment:
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_password
      POSTGRES_DB: test_db
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 768MB
      POSTGRES_WORK_MEM: 16MB
      POSTGRES_MAINTENANCE_WORK_MEM: 64MB   # Para operaciones de mantenimiento
      # Configuración específica para workers
      POSTGRES_MAX_CONNECTIONS: 50
      POSTGRES_MAX_WORKER_PROCESSES: 4
      POSTGRES_MAX_PARALLEL_WORKERS: 4
      POSTGRES_MAX_PARALLEL_WORKERS_PER_GATHER: 2
    volumes:
      - worker_data:/var/lib/postgresql/data
    depends_on:
      - consul
      - coordinator
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '1.0'      # Un núcleo máximo para workers
          memory: '1G'
        reservations:
          cpus: '0.25'     # Garantiza 25% de un núcleo
          memory: '512M'
      update_config:
        parallelism: 1
        delay: 60s
        order: start-first
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.role != manager  # Workers en nodos worker
    command: >
      postgres 
      -c config_file=/etc/postgresql/postgresql.conf
      -c maintenance_work_mem=64MB
      -c synchronous_commit=off
      -c checkpoint_timeout=30min
      -c max_wal_size=1GB
    healthcheck:
      test: >
        CMD-SHELL
        pg_isready -U test_user -d test_db &&
        psql -U test_user -d test_db -c "SELECT 1 FROM pg_extension WHERE extname = 'citus'"
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s  # Dar tiempo suficiente para el inicio inicial

  nginx:
    image: nginx:latest
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/conf/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/html:/usr/share/nginx/html:ro
      - ./nginx/static:/usr/share/nginx/static:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    networks:
      - default
    environment:
      - NGINX_ENTRYPOINT_QUIET_LOGS=1
    depends_on:
      - front
      - back
      - consul
    healthcheck:
      test: [ "CMD", "nginx", "-t" ]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      replicas: 1
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  redis:
      image: redis:latest
      healthcheck:
        test: [ "CMD", "redis-cli", "ping" ]
        interval: 10s
        timeout: 5s
        retries: 5
      networks:
        - default
      deploy:
        replicas: 1
      depends_on:
        - consul
      logging:
        driver: "json-file"
        options:
          max-size: "10m"
          max-file: "3"

  prometheus:
      image: prom/prometheus:latest
      env_file:
        - .env
      ports:
        - "9090:9090"
      volumes:
        - ./prometheus:/etc/prometheus:ro
        - prometheus-data:/prometheus
      networks:
        - default
      depends_on:
        - back
        - consul
      deploy:
        replicas: 1
      logging:
        driver: "json-file"
        options:
          max-size: "10m"
          max-file: "3"

  grafana:
      image: grafana/grafana:latest
      env_file:
        - .env
      ports:
        - "3000:3000"
      environment:
        - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      networks:
        - default
      volumes:
        - grafana-data:/var/lib/grafana
      depends_on:
        - prometheus
        - consul
      deploy:
        replicas: 1
      logging:
        driver: "json-file"
        options:
          max-size: "10m"
          max-file: "3"

  falco:
      image: falcosecurity/falco:latest
      privileged: true
      volumes:
        - /var/run/docker.sock:/var/run/docker.sock:ro
        - /proc:/host/proc:ro
        - /sys/kernel/debug:/sys/kernel/debug
        - /dev:/host/dev:ro
        - /etc:/host/etc:ro
        - /var/lib/docker:/var/lib/docker:ro
        - /run/containerd:/run/containerd:ro
      security_opt:
        - no-new-privileges:false
      environment:
        - FALCO_BPF_PROBE=""  # Forzar uso de eBPF
      command: >
        falco
        --cri /run/containerd/containerd.sock
        --driver bpf
        --modern-bpf
        -o json_output=true
        --stdout
        --severity=info
      depends_on:
        - consul
      networks:
        - default
      logging:
        driver: "json-file"
        options:
          max-size: "10m"
          max-file: "3"

volumes:
  vault-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  db-data:
    driver: local
  consul-data:
    driver: local
  coordinator_data:
    driver: local
  worker_data:
    driver: local

networks:
  default:
    driver: overlay
    ipam:
      config:
        - subnet: 10.0.0.0/24
          gateway: 10.0.0.1
    attachable: true