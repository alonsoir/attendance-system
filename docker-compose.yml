services:
  consul:
    image: hashicorp/consul:1.20
    container_name: attendance-consul
    env_file:
      - .env
    environment:
      - CONSUL_LOCAL_CONFIG={"verify_incoming":false}
    ports:
      - "8500:8500"
    networks:
      - default
    volumes:
      - consul-data:/consul/data
    command: agent -server -bootstrap -ui -bind=10.0.0.2 -client=0.0.0.0 -advertise=10.0.0.2
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8500/v1/status/leader || exit 1" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: '256M'
          cpus: '0.3'
        reservations:
          memory: '128M'
          cpus: '0.1'

  vault:
      image: hashicorp/vault:1.18
      container_name: attendance-vault
      restart: always
      env_file:
        - .env
      ports:
        - "8200:8200"
      environment:
        VAULT_API_ADDR: http://vault:8200
        VAULT_CLUSTER_ADDR: https://vault:8201
        VAULT_STORAGE: consul
        VAULT_BACKEND: consul
        VAULT_STORAGE_CONSUL_ADDRESS: http://consul:8500
      volumes:
        - ${PROJECT_ROOT}/backend/scripts:/scripts:ro
        - vault-data:/vault/data
      command: sh -c "sleep 10 && vault server -config=/scripts/vault.hcl"
      depends_on:
        - consul
      healthcheck:
        test: [ "CMD", "vault", "status" ]
        interval: 10s
        timeout: 5s
        retries: 3
        start_period: 30s
      cap_add:
        - IPC_LOCK
      networks:
        - default
      deploy:
        resources:
          limits:
            memory: '512M'
            cpus: '0.5'
          reservations:
            memory: '256M'
            cpus: '0.2'

  vault-init:
      container_name: attendance-vault-init
      image: alpine:latest
      env_file:
        - .env
      depends_on:
        - vault
      environment:
        - VAULT_ADDR=http://vault:8200
        - CONSUL_ADDR=http://consul:8500
      networks:
        - default
      volumes:
        - .env:/app/.env:ro
        - /Users/aironman/git/attendance_system/backend/scripts:/scripts
        - vault-data:/vault/file
        - /Users/aironman/git/attendance_system/vault-init:/vault-init
      command: sh -c "chmod +x /scripts/simple-init-vault.sh && /scripts/simple-init-vault.sh"

  front:
    build:
      context: .
      dockerfile: attendance_system/frontend/Dockerfile
    container_name: attendance-front
    image: frontend-attendance
    env_file:
      - .env
    environment:
      - VAULT_ADDR=http://vault:8200
      - VAULT_TOKEN=attendance_root_token
      - VITE_API_URL=http://back:8000
    networks:
      - default
    depends_on:
      - back
      - consul
      - vault
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/health", "||", "exit", "1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    deploy:
      replicas: 1
      resources:
        limits:
          memory: '512M'
          cpus: '0.5'
        reservations:
          memory: '256M'
          cpus: '0.2'
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      - ./logs:/app/logs:rw

  back:
    build:
      context: .
      dockerfile: attendance_system/backend/Dockerfile
    container_name: attendance-back
    image: backend-attendance
    env_file:
      - .env
    environment:
      - POSTGRES_SERVER=coordinator
      - POSTGRES_PORT=5433
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - VAULT_ADDR=http://vault:8200
      - VAULT_TOKEN=attendance_root_token
      - PROJECT_NAME=${PROJECT_NAME}
      - PROJECT_DESCRIPTION=${PROJECT_DESCRIPTION}
      - VERSION=${VERSION}
      - API_V1_STR=${API_V1_STR}
      - BACKEND_CORS_ORIGINS=${BACKEND_CORS_ORIGINS}
      - ENABLE_METRICS=${ENABLE_METRICS}
      - SECRET_KEY=${SECRET_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    networks:
      - default
    depends_on:
      - vault
      - consul
      - coordinator
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health", "||", "exit", "1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      replicas: 1
      resources:
        limits:
          memory: '1G'
          cpus: '1.0'
        reservations:
          memory: '512M'
          cpus: '0.5'
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      - ./logs:/app/logs:rw

  coordinator:
    image: test-postgres14-full-citus12:latest
    container_name: attendance-coordinator
    environment:
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_password
      POSTGRES_DB: test_db
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 768MB
      POSTGRES_WORK_MEM: 16MB
      POSTGRES_MAINTENANCE_WORK_MEM: 64MB
      POSTGRES_MAX_CONNECTIONS: 100
      POSTGRES_MAX_WORKER_PROCESSES: 8
      POSTGRES_MAX_PARALLEL_WORKERS: 8
      POSTGRES_MAX_PARALLEL_WORKERS_PER_GATHER: 4
    networks:
      - default
    ports:
      - "5433:5432"
    volumes:
      - coordinator_data:/var/lib/postgresql/data
    depends_on:
      #worker:
      #  condition: service_healthy
      - consul
      #  condition: service_healthy
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '1.5'
          memory: '1G'
        reservations:
          cpus: '0.5'
          memory: '512M'
      placement:
        constraints:
          - node.role == manager
      update_config:
        parallelism: 1
        delay: 120s
        order: start-first
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
    command: >
      postgres 
      -c config_file=/etc/postgresql/postgresql.conf
      -c maintenance_work_mem=128MB
      -c synchronous_commit=off
      -c checkpoint_timeout=30min
      -c max_wal_size=2GB
    healthcheck:
      test: >
        CMD-SHELL
        pg_isready -U test_user -d test_db &&
        psql -U test_user -d test_db -c "SELECT 1 FROM citus_dist_local_group" &&
        psql -U test_user -d test_db -c "SELECT * FROM citus_health_check()"
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s

  worker:
    image: test-postgres14-full-citus12:latest
    container_name: attendance-worker
    environment:
      POSTGRES_USER: test_user
      POSTGRES_PASSWORD: test_password
      POSTGRES_DB: test_db
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_EFFECTIVE_CACHE_SIZE: 768MB
      POSTGRES_WORK_MEM: 16MB
      POSTGRES_MAINTENANCE_WORK_MEM: 64MB
      POSTGRES_MAX_CONNECTIONS: 50
      POSTGRES_MAX_WORKER_PROCESSES: 4
      POSTGRES_MAX_PARALLEL_WORKERS: 4
      POSTGRES_MAX_PARALLEL_WORKERS_PER_GATHER: 2
      POSTGRES_HOST_AUTH_METHOD: trust
      POSTGRES_LISTEN_ADDRESSES: '*'
    networks:
      - default
    ports:
      - "5432"
    volumes:
      - worker_data:/var/lib/postgresql/data
    depends_on:
      - consul
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '1.0'
          memory: '1G'
        reservations:
          cpus: '0.25'
          memory: '512M'
      update_config:
        parallelism: 1
        delay: 60s
        order: start-first
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      placement:
        constraints:
          - node.role != manager
    command: >
      postgres 
      -c config_file=/etc/postgresql/postgresql.conf
      -c listen_addresses='*'
      -c port=5432
      -c maintenance_work_mem=64MB
      -c synchronous_commit=off
      -c checkpoint_timeout=30min
      -c max_wal_size=1GB
    healthcheck:
      test: >
        CMD-SHELL
            pg_isready -U test_user -d test_db &&
            psql -U test_user -d test_db -c "SELECT 1 FROM pg_extension WHERE extname = 'citus'" &&
            psql -U test_user -d test_db -c "SELECT pg_is_ready_to_accept_connections()"
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  nginx:
    image: nginx:latest
    container_name: attendance-nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/conf/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf/conf.d:/etc/nginx/conf.d:ro
      - ./nginx/html:/usr/share/nginx/html:ro
      - ./nginx/static:/usr/share/nginx/static:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    networks:
      - default
    environment:
      - NGINX_ENTRYPOINT_QUIET_LOGS=1
    depends_on:
      - front
      - back
      - consul
    healthcheck:
      test: [ "CMD", "nginx", "-t" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      replicas: 1
      resources:
        limits:
          memory: '256M'
          cpus: '0.3'
        reservations:
          memory: '128M'
          cpus: '0.1'
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  redis:
    image: redis:latest
    container_name: attendance-redis
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - default
    deploy:
      replicas: 1
      resources:
        limits:
          memory: '256M'
          cpus: '0.3'
        reservations:
          memory: '128M'
          cpus: '0.1'
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    depends_on:
      - consul
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  prometheus:
    image: prom/prometheus:latest
    container_name: attendance-prometheus
    env_file:
      - .env
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus:/etc/prometheus:ro
      - prometheus-data:/prometheus
    networks:
      - default
    depends_on:
      - back
      - consul
    deploy:
      replicas: 1
      resources:
        limits:
          memory: '512M'
          cpus: '0.5'
        reservations:
          memory: '256M'
          cpus: '0.2'
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  grafana:
    image: grafana/grafana:latest
    container_name: attendance-grafana
    env_file:
      - .env
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
    networks:
      - default
    volumes:
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
      - consul
    deploy:
      replicas: 1
      resources:
        limits:
          memory: '512M'
          cpus: '0.5'
        reservations:
          memory: '256M'
          cpus: '0.2'
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  falco:
    image: falcosecurity/falco:latest
    container_name: attendance-falco
    privileged: true
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /proc:/host/proc:ro
      - /sys/kernel/debug:/sys/kernel/debug
      - /dev:/host/dev:ro
      - /etc:/host/etc:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /run/containerd:/run/containerd:ro
    security_opt:
      - no-new-privileges:false
    environment:
      - FALCO_BPF_PROBE=""
    command: >
      falco
      --cri /run/containerd/containerd.sock
      --driver bpf
      --modern-bpf
      -o json_output=true
      --stdout
      --severity=info
    depends_on:
      - consul
    networks:
      - default
    deploy:
      resources:
        limits:
          memory: '512M'
          cpus: '0.5'
        reservations:
          memory: '256M'
          cpus: '0.2'
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  vault-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  consul-data:
    driver: local
  coordinator_data:
    driver: local
  worker_data:
    driver: local

networks:
  default:
    driver: overlay
    ipam:
      config:
        - subnet: 10.0.0.0/24
          gateway: 10.0.0.1
    attachable: true